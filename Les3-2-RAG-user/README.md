# RAG user
æœ¬ãƒ‘ãƒ¼ãƒˆã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚„å…¥åŠ›ã‚’å—ã‘å–ã‚Šã€ãã‚Œã‚‰ã‚’åŸ‹ã‚è¾¼ã‚€äº‹ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä½•ã‚’æœ›ã‚“ã§ã„ã‚‹ã®ã‹ã®ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã‚’å¾—ã‚‹.  
ãã—ã¦ã€ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‹ã‚‰è³ªå•ãƒ™ã‚¯ãƒˆãƒ«ã®è¿‘ãã«ã‚ã‚‹é–¢é€£ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—ã™ã‚‹.  
ãã®ã‚ã¨ã«ã€ãã‚Œã‚‰ã®æ›¸é¡ã‚’å–ã‚Šå‡ºã—ã€ãƒãƒ£ãƒ³ã‚¯ã‚’è£œå¼·ã—ã¦LLMã«è¿”ã™å‡¦ç†ã‚’ã™ã‚‹.  

LangChainã¯ã“ã‚Œã‚‰ã®å‡¦ç†ã‚’ãŸã£ãŸä¸€è¡Œã§ã—ã¦ãã‚Œã‚‹. ã“ã‚ŒãŒLangChainã®å¼·ã¿ã§ã‚ã‚‹.  

## Code
```python
retrieval_qa_chat_prompt = hub.pull("langchain-ai/retrieval-qa-chat")
combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)
retrival_chain = create_retrieval_chain(
    retriever=vectorstore.as_retriever(), combine_docs_chain=combine_docs_chain
)

result = retrival_chain.invoke(input={"input": query})

```
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ  
	- ã€Œlangchain-ai/retrieval-qa-chatã€  
		```txt
		Answer any use questions based solely on the context below:

		<context>
		{context}
		</context>
		```
		Contextã«ã¯ã€äººé–“ã®å…¥åŠ›ã‚„ãƒãƒ£ãƒƒãƒˆãƒ’ã‚¹ãƒˆãƒªãƒ¼ãŒå«ã¾ã‚Œã‚‹.  
		ã“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’LLMã«é€ã‚‹ã¨ã€LangChainã¯ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã§ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‹ã‚‰å¾—ãŸé–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å…¥åŠ›ã™ã‚‹.  
		
		LLMã¯çµ±è¨ˆçš„ãªæ¨è«–ãªã®ã§ã€æ–‡è„ˆã«æ²¿ã£ãŸå›ç­”ã®ã¿ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ°´å¢—ã—ã™ã‚‹æ–¹æ³•ã¯å¿…ãšã—ã‚‚å½¹ã«ç«‹ãŸãªã„.    

	- custom  
		```python
		rag_chain = (
			{"context": vectorstore.as_retriever() | format_docs, "question": RunnablePassthrough()}
			| custom_rag_prompt
			| llm
		)
		```

		- [RunnablePassthrough](https://python.langchain.com/v0.1/docs/expression_language/primitives/passthrough/)  
			ã“ã“ã§ã¯ã€questionã‚­ãƒ¼ã«å¼•æ•°ã¨ã—ã¦ã€è³ªå•ã‚’æ¸¡ã™äº‹ã‚’æ„å‘³ã—ã¦ã„ã‚‹.  
			ãã®ãŸã‚ã€è³ªå•ã®å€¤ã«ã¯æœ€çµ‚çš„ãªLLMã®å‘¼ã³å‡ºã—ã«åæ˜ ã•ã‚Œã‚‹.  

		- vectorstore.as_retriever() | format_docs  
			ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‹ã‚‰è³ªå•ã«é–¢é€£ã™ã‚‹æ›¸é¡ã‚’å—ã‘å–ã‚Šã€ãã‚Œã‚‰ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã—ã¦å–å¾—ã—ã¦ã„ã‚‹.  
			

- create_stuff_documents_chain  
    ã“ã®é–¢æ•°ã¯ã€å…¥åŠ›ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒªã‚¹ãƒˆã¨ã—ã¦å—ã‘å–ã‚Šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹.  
    [LINK](https://python.langchain.com/v0.1/docs/use_cases/chatbots/retrieval/#document-chains)

- create_retrieval_chain
    ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã—ã€å…¨ã¦ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å—ã‘å–ã‚Šã€å˜ã«ã™ã¹ã¦ã‚’ä¸€ç·’ã«è©°ã‚ã‚‹.  
    ã“ã‚Œã«ã‚ˆã£ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ãQ&AãŒå¯èƒ½ãªã®ã§ã‚ã‚‹.  

### å®Ÿè¡Œçµæœ
```bash
{
	'input': 'what is Pinecone in machine learning?',
	 'context': [
		Document(id='18ae6a94-6dc3-44c7-9b3a-277146ec816d',
		 metadata={
			'source': 'mediumblog1.txt'
		},
		 page_content='Pinecone is designed to be fast and scalable,
		 allowing for efficient retrieval of similar data points based on their vector representations.\nIt can handle large-scale ML applications with millions or billions of data points.\nPinecone provides infrastructure management or maintenance to its users.\nPinecone can handle high query throughput and low latency search.\nPinecone is a secure platform that meets the security needs of businesses and organizations.\nPinecone is designed to be user-friendly and accessible via its simple API for storing and retrieving vector data,
		 making it easy to integrate into existing ML workflows.\nPinecone supports real-time updates,
		 allowing for efficient updates to the vector database as new data points are added. This ensures that the vector database remains up-to-date and accurate over time.\nPinecone can be synced with data from various sources using tools like Airbyte and monitored using Datadog\nChroma DB\nChroma DB is an open-source vector store for storing and retrieving vector embeddings. It is mainly used to save embeddings along with metadata to be used later by LLMs and can also be used for semantic search engines over text data.'),
		 Document(id='c088f890-30f1-4aa9-b3d9-f8d25dd2235b',
		 metadata={
			'source': 'mediumblog1.txt'
		},
		 page_content='Weaviate can store and search vectors from various data modalities,
		 including images,
		 text,
		 and audio.\nWeaviate provides seamless integration with machine learning frameworks such as Hugging Face,
		 Open AI,
		 LangChain,
		 Llamaindex,
		 TensorFlow,
		 PyTorch,
		 and Scikit-learn.\nWeaviate can index vectors in real-time,
		 making it ideal for applications that require low-latency search.\nWeaviate can be scaled to handle large volumes of data and high query throughput.\nWeaviate can be used in memory for fast search or with disk-based storage for larger datasets.\nWeaviate provides a user-friendly interface for managing vectors and performing searches.\nPinecone\nPinecone is a fully managed cloud-based vector database that is designed to make it easy for businesses and organizations to build and deploy large-scale ML applications.\n\n\nSome Pinecone Features : '),
		 Document(id='cd7e1c01-4252-4418-bfe2-4dc6d86afc18',
		 metadata={
			'source': 'mediumblog1.txt'
		},
		 page_content='ChatGPT prompts\n47 stories\nÂ·\n1439 saves\nSparse embedding or BM25?\nInfiniFlow\nInfiniFlow\n\nSparse embedding or BM25?\nSince the open-sourcing of Infinity,
		 it has received a wide positive response from the community. Regarding the essential RAG technology weâ€¦\n7 min read\nÂ·\nFeb 13,
		 2024\n16\n\nExperimenting with Vector Databases: Chromadb,
		 Pinecone,
		 Weaviate and Pgvector\nVishnu Sivan\nVishnu Sivan\n\nin\n\nCoinsBench\n\nExperimenting with Vector Databases: Chromadb,
		 Pinecone,
		 Weaviate and Pgvector\nVector databases are specialized systems designed for storing,
		 managing,
		 and searching embedding vectors. The widespread adoption ofâ€¦\n10 min read\nÂ·\nNov 14,
		 2023\n164\n\n1\n\nğŸš€ Blazing Fast Text Embeddings Inference for your RAG\nDavid Min\nDavid Min\n\nğŸš€ Blazing Fast Text Embeddings Inference for your RAG\nHugging Face\u200aâ€”\u200aText Embeddings Inference (TEI)\n6 min read\nÂ·\nOct 21,
		 2023\n94\n\n2\n\nVector Database\u200aâ€”\u200aIntroduction and Python Implementation\nDenaya\nDenaya'),
		 Document(id='b80cf6b5-66cb-45f0-9bc4-2f09e297669f',
		 metadata={
			'source': 'mediumblog1.txt'
		},
		 page_content='Weaviate\nPinecone\nChroma DB\nQdrant\nMilvus\nHereâ€™s an overview of some of the features of these vector databases. You can go see this comprehensive vector database features matrix by Dhruv Anand\n\n\nSource: Author\nWeaviate\nWeaviate is an open-source vector database that can be used to store,
		 search,
		 and manage vectors of any dimensionality. It is designed to be scalable and easy to use,
		 and it can be deployed on-premises or in the cloud.\n\n\nFeatures : ')
	],
	 'answer': 'Pinecone is a fully managed cloud-based vector database designed to facilitate the building and deployment of large-scale machine learning (ML) applications. It is optimized for fast and scalable retrieval of similar data points based on their vector representations. Pinecone can handle millions or billions of data points,
	 accommodates high query throughput,
	 and ensures low-latency search. The platform provides infrastructure management and maintenance,
	 real-time updates,
	 and easy integration into existing ML workflows via a simple API. Additionally,
	 Pinecone supports syncing with various data sources and monitoring,
	 making it a secure and user-friendly option for businesses and organizations.'
}
```